{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helper import TimeSeriesDataset\n",
    "from model import DARNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import wandb\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import copy\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from helper import prepare_data_with_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Replace YOUR_API_KEY with your actual API key\n",
    "# os.environ['WANDB_API_KEY'] = 'b62c5332280f3f3dec6354dacedf36d490db3aeb'\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 1\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "dir_path = '/Users/charlesmiller/Documents/Code/DL_experiments/project_c/models/DARNN/data'\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_model(params,features_cont,features_cat,save_model = False,model_name = 'darnn_idxmag7'):\n",
    "    model_params = {\n",
    "        'input_features':  \"num of features\",\n",
    "        'encoder_units':  64,\n",
    "        'decoder_units': 64,\n",
    "        'time_steps':  \"time_steps\",\n",
    "        'device': device,\n",
    "    }\n",
    "    model = DARNN(**model_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_log(loss, example_ct, epoch):\n",
    "    # Where the magic happens\n",
    "    # wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
    "    print(f\"Loss after {str(example_ct).zfill(5)} examples: {loss:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, config, save_model = True, model_name = 'best_model'):\n",
    "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
    "    # wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
    "    model = model.to(device)\n",
    "    X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "    X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "    training_loss = []\n",
    "    validation_loss = []\n",
    "    example_ct = 0\n",
    "    min_val_loss = 100000\n",
    "    batch_size = 128\n",
    "    epochs = config['epochs']\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    # val_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            # Move tensors to the right device\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            prediction = model(inputs)\n",
    "            loss = criterion(prediction, labels)\n",
    "            example_ct +=  len(labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            val_prediction = model(inputs)\n",
    "            val_loss = criterion(val_prediction, labels)\n",
    "\n",
    "            training_loss.append(loss.item())\n",
    "            validation_loss.append(val_loss.item())\n",
    "\n",
    "        if val_loss.item() < min_val_loss:\n",
    "            best_params = copy.deepcopy(model.state_dict())\n",
    "            min_val_loss = val_loss.item()\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}: Train loss: {loss.item()}, Validation loss: {val_loss.item()}\")\n",
    "            # train_log(loss, example_ct, epoch)\n",
    "\n",
    "    if save_model:\n",
    "        torch.save(best_params, f'{dir_path}/{model_name}.pth')\n",
    "\n",
    "    return min_val_loss\n",
    "\n",
    "def build_optimizer(model, config):\n",
    "    if config['optimizer'] == 'adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=config['learning_rate'])\n",
    "    elif config['optimizer'] == 'sgd':\n",
    "        optimizer = torch.optim.SGD(\n",
    "            model.parameters(), lr=config['learning_rate'])\n",
    "    return optimizer\n",
    "\n",
    "def make(config,features_cont,features_cat):\n",
    "    # df = pd.read_csv(data_path)\n",
    "    # alerts_df = pd.read_csv(alerts_path)\n",
    "    # df = df.sort_values('t',ascending=True).reset_index(drop=True)\n",
    "    # train_idx = int(len(df) * 0.8)\n",
    "    # train = df.iloc[:train_idx]\n",
    "    # test = df.iloc[train_idx:]\n",
    "    # X_train, y_train = prepare_data(train,features_cont,features_cat,window_size=config['window_size'],alerts_df=alerts_df)\n",
    "    # X_test, y_test = prepare_data(test,features_cont,features_cat,window_size=config['window_size'],alerts_df=alerts_df)\n",
    "    model = prepare_model(config, features_cont, features_cat)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = build_optimizer(model, config)\n",
    "    \n",
    "    return model, criterion, optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test(config):\n",
    "#     location = 'Sydney'\n",
    "#     w = 14\n",
    "#     # We pick 14 before the 2016-01-01 for the sliding window\n",
    "#     from_date = '2015-12-17'\n",
    "#     to_date = '2017-01-01'\n",
    "\n",
    "#     date_fmt = '%Y-%m-%d'\n",
    "#     df = get_df_complete()\n",
    "\n",
    "#     # features\n",
    "#     features_cont = ['MinTemp', 'MaxTemp', 'Rainfall', 'WindGustSpeed', 'WindSpeed9am',\n",
    "#                     'WindSpeed3pm', 'Humidity9am', 'Humidity3pm', 'Pressure9am',\n",
    "#                     'Pressure3pm', 'Cloud9am', 'Cloud3pm', 'Temp9am', 'Temp3pm']\n",
    "#     features_cat = ['RainToday']\n",
    "\n",
    "#     # Assuming df is your DataFrame\n",
    "\n",
    "\n",
    "#     X, Y = [], []\n",
    "#     df_l = df[(df['Location'] == location) & (df.index < to_date) & (df.index > from_date)]\n",
    "#     D = []\n",
    "#     for f in features_cont:\n",
    "#         D.append(df_l[f].interpolate('linear').fillna(0).values)\n",
    "#     for f in features_cat:\n",
    "#         D.append(df_l[f].map({'Yes': 1, 'No': 0}).fillna(0).values)\n",
    "#         # transpose to time series\n",
    "#     TS = []\n",
    "#     for i in range(df_l.shape[0]):\n",
    "#         row = []\n",
    "#         for c in D:\n",
    "#             row.append(c[i])\n",
    "#         TS.append(row)\n",
    "#     in_seq, out_seq = sliding_window(TS, w, 1)\n",
    "#     rain_seq = [r[0][-1] for r in out_seq]\n",
    "#     X.extend(in_seq)\n",
    "#     Y.extend(rain_seq)\n",
    "\n",
    "#     # X[features_cont] = scaler.fit_transform(X[features_cont])\n",
    "#     X_test = torch.tensor(X).float().transpose(1, 2)\n",
    "\n",
    "#     model_name = 'best_model'\n",
    "#     model = prepare_model(config, features_cont, features_cat, model_name = model_name)\n",
    "#     model = model.to(device)\n",
    "#     model.load_state_dict(torch.load(f'{dir_path}/{model_name}.pth'))\n",
    "#     model.eval()\n",
    "#     X_test = X_test.to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         predicted_prob = model(X_test)\n",
    "#         model_prediction = predicted_prob.data.max(1, keepdim = True)[1].view(-1).tolist()\n",
    "#         no_rain_prediction = [0] * len(Y)\n",
    "#         no_sun_prediction = [1] * len(Y)\n",
    "#         coin_flip_prediction = [random.randint(0, 1) for _ in range(len(Y))]\n",
    "#         tomorrow_like_today_prediction = X_test[:, -1, -1].view(-1).tolist()\n",
    "\n",
    "#         ba_sc = balanced_accuracy_score\n",
    "#         model_score = round(ba_sc(Y, model_prediction), 4)\n",
    "#         no_rain_score = round(ba_sc(Y, no_rain_prediction), 4)\n",
    "#         no_sun_score = round(ba_sc(Y, no_sun_prediction), 4)\n",
    "#         coin_flip_score = round(ba_sc(Y, coin_flip_prediction), 4)\n",
    "#         tlt_score = round(ba_sc(Y, tomorrow_like_today_prediction), 4)\n",
    "\n",
    "#         print(f'Model Prediction Score: {model_score}')\n",
    "#         print(f'No Rain Prediction Score: {no_rain_score}')\n",
    "#         print(f'Rain Prediction Score: {no_sun_score}')\n",
    "#         print(f'Coin Flip Prediction Score: {coin_flip_score}')\n",
    "#         print(f'Tomorrow like Today Prediction Score: {tlt_score}')\n",
    "\n",
    "#     torch.onnx.export(model, X_test, \"model.onnx\")\n",
    "#     wandb.save(\"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pipeline(hyperparameters,features_cont,features_cat,X_train, y_train, X_test, y_test):\n",
    "\n",
    "    # tell wandb to get started\n",
    "    with wandb.init(project=\"darnn-idxmag7\", config=hyperparameters):\n",
    "      # access all HPs through wandb.config, so logging matches execution!\n",
    "      config = wandb.config\n",
    "\n",
    "    # make the model, data, and optimization problem\n",
    "    model,criterion, optimizer = make(hyperparameters,features_cont,features_cat)\n",
    "    print(model)\n",
    "\n",
    "    # and use them to train the model\n",
    "    train_model(model, X_train, y_train, X_test, y_test, criterion, optimizer, hyperparameters)\n",
    "\n",
    "    # # and test its final performance\n",
    "    test(config)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"tcl_num\":          2,\n",
    "    \"tcl_channel_size\": 128,\n",
    "    \"kernel_size\":      7,\n",
    "    \"dropout\":          0.3,\n",
    "    \"slices\":           1,\n",
    "    \"use_bias\":         True,\n",
    "    \"learning_rate\":    0.01,\n",
    "    \"epochs\": 800,\n",
    "    \"classes\":2,\n",
    "    \"batch_size\": 128,\n",
    "    \"window_size\": 20,\n",
    "    \"dataset\" : \"BFC3D_CLASSIFIER\",\n",
    "    \"architecture\": \"TCN\",\n",
    "    \"optimizer\": \"adam\"\n",
    "  }\n",
    "features_cont = ['v', 'vw', 'o', 'c', 'h', 'l','n','sma_20','sma_5', 'bb_spread',\n",
    "       'bb_trend','sma_20_trend', 'sma_5_trend', 'pct_5d_high','rsi','macd','roc',\n",
    "       'pct_5d_low', 'stddev_close_diff_5d', 'stddev_close_diff_10d']\n",
    "features_cat = ['bb_category']\n",
    "features = features_cont + features_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_data = pd.read_csv('data_window20.csv')\n",
    "identifiers = windowed_data['alert_identifier'].unique()\n",
    "train_identifiers = identifiers[:int(len(identifiers)*0.8)]\n",
    "test_identifiers = identifiers[int(len(identifiers)*0.8):]\n",
    "train = windowed_data[windowed_data['alert_identifier'].isin(train_identifiers)]\n",
    "test = windowed_data[windowed_data['alert_identifier'].isin(test_identifiers)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105734 26434\n"
     ]
    }
   ],
   "source": [
    "print(len(train_identifiers),len(test_identifiers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(train[features_cont])\n",
    "# train[features_cont] = scaler.transform(train[features_cont])\n",
    "# test[features_cont] = scaler.transform(test[features_cont])\n",
    "\n",
    "# X_test, y_test = [], []\n",
    "# print('Preparing test data')\n",
    "# for identifier in test_identifiers[:5000]:\n",
    "#     X_test.append(test[test['alert_identifier'] == identifier][features].values)\n",
    "#     y_test.append(test[test['alert_identifier'] == identifier]['one_max_vol_label'].values[-1])\n",
    "\n",
    "# X_train, y_train = [], []\n",
    "# print('Preparing train data')\n",
    "# for identifier in train_identifiers[70000:]:\n",
    "#     X_train.append(train[train['alert_identifier'] == identifier][features].values)\n",
    "#     y_train.append(train[train['alert_identifier'] == identifier]['one_max_vol_label'].values[-1])\n",
    "\n",
    "# X_val, y_val = [], []\n",
    "# print('Preparing val data')\n",
    "# for identifier in test_identifiers[5000:6000]:\n",
    "#     X_val.append(train[train['alert_identifier'] == identifier][features].values)\n",
    "#     y_val.append(train[train['alert_identifier'] == identifier]['one_max_vol_label'].values[-1])\n",
    "\n",
    "X_train = torch.tensor(X_train).float().transpose(1, 2)\n",
    "y_train = torch.tensor(y_train).long()\n",
    "X_test = torch.tensor(X_test).float().transpose(1, 2)\n",
    "y_test = torch.tensor(y_test).long()\n",
    "# X_val = torch.tensor(X_val).float().transpose(1, 2)\n",
    "# y_val = torch.tensor(y_val).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([35734, 21, 20])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DARNN(X_train.shape[2], 128, 128, X_train.shape[1],num_classes=2,device=device).to(device)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_scheduler = torch.optim.lr_scheduler.StepLR(opt, 20, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, average_precision_score\n",
    "data_train_loader = DataLoader(TensorDataset(X_train, y_train), shuffle=True, batch_size=128)\n",
    "data_test_loader = DataLoader(TensorDataset(X_test, y_test), shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The next step is to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving...\n",
      "Iter:  0 train:  0.8153850864846838 val:  0.803644207626218\n",
      "acc:  0.6336 bal_acc:  0.5201843086632243\n",
      "Iter:  1 train:  0.8056162789895439 val:  0.8071147073317201\n",
      "Saving...\n",
      "Iter:  2 train:  0.8030258635978527 val:  0.8003069533126455\n",
      "Iter:  3 train:  0.8025087482487684 val:  0.8267017948342027\n",
      "Saving...\n",
      "Iter:  4 train:  0.7991951865782567 val:  0.7991580325692144\n",
      "Iter:  5 train:  0.7978153617077464 val:  0.8150436815596706\n",
      "Iter:  6 train:  0.7976817243107028 val:  0.8078819908490058\n",
      "Iter:  7 train:  0.8053841790261644 val:  0.8057676388203242\n",
      "Iter:  8 train:  0.8461429941760205 val:  0.8057677259303327\n",
      "Iter:  9 train:  0.8461271825687084 val:  0.8057677259303327\n",
      "Iter:  10 train:  0.8460448002080194 val:  0.8056489944140174\n",
      "acc:  0.664 bal_acc:  0.5\n",
      "Iter:  11 train:  0.953022545955627 val:  0.9885654394884236\n",
      "Iter:  12 train:  0.954249391858971 val:  0.9885654394884236\n",
      "Iter:  13 train:  0.9542493918432391 val:  0.9885654394884236\n",
      "Iter:  14 train:  0.954249391848483 val:  0.9885654394884236\n",
      "Iter:  15 train:  0.9542493918537269 val:  0.9885654394884236\n",
      "Iter:  16 train:  0.954249391851979 val:  0.9885654394884236\n",
      "Iter:  17 train:  0.954249391851979 val:  0.9885654394884236\n",
      "Iter:  18 train:  0.954249391851979 val:  0.9885654394884236\n"
     ]
    }
   ],
   "source": [
    "epochs = 550\n",
    "loss = nn.CrossEntropyLoss()\n",
    "patience = 15\n",
    "min_val_loss = 9999\n",
    "counter = 0\n",
    "for i in range(epochs):\n",
    "    mse_train = 0\n",
    "    for batch_x, batch_y in data_train_loader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        opt.zero_grad()\n",
    "        y_pred = model(batch_x)\n",
    "        y_pred = y_pred.squeeze(1)\n",
    "        l = loss(y_pred, batch_y)\n",
    "        l.backward()\n",
    "        mse_train += l.item()*batch_x.shape[0]\n",
    "        opt.step()\n",
    "    epoch_scheduler.step()\n",
    "    with torch.no_grad():\n",
    "        mse_val = 0\n",
    "        preds = []\n",
    "        true = []\n",
    "        for batch_x, batch_y in data_test_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            output = model(batch_x)\n",
    "            output = output.squeeze(1)\n",
    "            preds.append(output.detach().cpu().numpy())\n",
    "            true.append(batch_y.detach().cpu().numpy())\n",
    "            mse_val += loss(output, batch_y).item()*batch_x.shape[0]\n",
    "    preds = np.concatenate(preds)\n",
    "    true = np.concatenate(true)\n",
    "    \n",
    "    if min_val_loss > mse_val**0.5:\n",
    "        min_val_loss = mse_val**0.5\n",
    "        print(\"Saving...\")\n",
    "        torch.save(model.state_dict(), \"darnn_nasdaq.pt\")\n",
    "        counter = 0\n",
    "    else: \n",
    "        counter += 1\n",
    "    \n",
    "    if counter == patience:\n",
    "        break\n",
    "    print(\"Iter: \", i, \"train: \", (mse_train/len(X_train))**0.5, \"val: \", (mse_val/len(X_test))**0.5)\n",
    "    if(i % 10 == 0):\n",
    "        ## need to add accuracy\n",
    "        # preds = preds*(target_train_max - target_train_min) + target_train_min\n",
    "        # true = true*(target_train_max - target_train_min) + target_train_min\n",
    "        # mse = mean_squared_error(true, preds)\n",
    "        # mae = mean_absolute_error(true, preds)\n",
    "        acc = accuracy_score(true, np.argmax(preds, axis=1))\n",
    "        bal_acc = balanced_accuracy_score(true, np.argmax(preds, axis=1))\n",
    "        print(\"acc: \", acc, \"bal_acc: \", bal_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.9999750e-01, 2.5486390e-06],\n",
       "       [9.9999940e-01, 6.3530064e-07],\n",
       "       [9.9999785e-01, 2.1878295e-06],\n",
       "       [9.9999797e-01, 2.0059467e-06],\n",
       "       [9.9999785e-01, 2.1814603e-06],\n",
       "       [9.9999762e-01, 2.3466964e-06],\n",
       "       [9.9999607e-01, 3.9053161e-06],\n",
       "       [9.9999940e-01, 6.4674191e-07],\n",
       "       [9.9999607e-01, 3.9166885e-06],\n",
       "       [9.9999774e-01, 2.2521695e-06],\n",
       "       [9.9999750e-01, 2.5363649e-06],\n",
       "       [9.9999940e-01, 6.4300025e-07],\n",
       "       [9.9999762e-01, 2.3483396e-06],\n",
       "       [9.9999940e-01, 6.3192942e-07],\n",
       "       [9.9999774e-01, 2.2386957e-06],\n",
       "       [9.9999654e-01, 3.4245973e-06],\n",
       "       [9.9999738e-01, 2.5630143e-06],\n",
       "       [9.9999607e-01, 3.9102320e-06],\n",
       "       [9.9999750e-01, 2.5217369e-06],\n",
       "       [9.9999177e-01, 8.2108199e-06],\n",
       "       [9.9999750e-01, 2.5439838e-06],\n",
       "       [9.9999785e-01, 2.1786659e-06],\n",
       "       [9.9999821e-01, 1.8032334e-06],\n",
       "       [9.9999833e-01, 1.7210220e-06],\n",
       "       [9.9999189e-01, 8.1449498e-06]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL_experiments-uYdK1qgD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
